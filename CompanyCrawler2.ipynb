{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests beautifulsoup4 PyPDF2 pdfplumber\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#import PyPDF2\n",
    "import io\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import re\n",
    "import logging\n",
    "import pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return io.BytesIO(response.content)\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error downloading PDF from {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def find_pdf_links(url, domain):\n",
    "    pdf_links = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if 'mailto:' in href or not href.lower().endswith('.pdf'):\n",
    "                    continue\n",
    "                pdf_link = urljoin(domain, href)\n",
    "                if domain in urlparse(pdf_link).netloc:\n",
    "                    pdf_links.append(pdf_link)\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Error finding PDF links on {url}: {e}\")\n",
    "    return pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search keywords in webpage content\n",
    "\n",
    "\n",
    "def search_keywords_in_webpage(html, keywords):\n",
    "    try:\n",
    "        # Convert the HTML to lowercase for case-insensitive searching\n",
    "        lower_html = html.lower()\n",
    "        for keyword in keywords:\n",
    "            # Use regular expression for precise word boundary matching\n",
    "            if re.search(r'\\b' + re.escape(keyword.lower()) + r'\\b', lower_html):\n",
    "                return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error searching keywords in webpage: {e}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def search_keywords_in_pdf(pdf_io, keywords):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_io) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in text.lower():\n",
    "                        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error searching keywords in PDF: {e}\")\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to crawl website\n",
    "def crawl_website(start_url, domain, keywords, max_depth=0):\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = {start_url}\n",
    "    found_keywords_summary = {}  # Dictionary to store summary of findings\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop()\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        # Check if the URL belongs to the original domain\n",
    "        if urlparse(current_url).netloc != urlparse(domain).netloc:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code == 200:\n",
    "                logging.info(f\"Crawling URL: {current_url}\")\n",
    "\n",
    "                # Search for keywords in the webpage\n",
    "                if search_keywords_in_webpage(response.text, keywords):\n",
    "                    logging.info(f\"Keyword hit found at {current_url}\")\n",
    "                    found_keywords_summary[current_url] = keywords\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    absolute_link = urljoin(domain, link['href'])\n",
    "\n",
    "                    # Check if the absolute link is within the original domain\n",
    "                    if urlparse(absolute_link).netloc == urlparse(domain).netloc:\n",
    "                        if absolute_link not in visited_urls:\n",
    "                            urls_to_visit.add(absolute_link)\n",
    "\n",
    "            # Process PDF links after processing all anchor tags\n",
    "            pdf_links = find_pdf_links(current_url, domain)           \n",
    "            for pdf_link in pdf_links:\n",
    "                pdf_io = download_pdf(pdf_link)\n",
    "                if pdf_io and search_keywords_in_webpage(pdf_io, keywords):\n",
    "                    logging.info(f\"Found keywords in PDF: {pdf_link}\")\n",
    "                    found_keywords_summary[pdf_link] = keywords\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error crawling {current_url}: {e}\")\n",
    "\n",
    "    return found_keywords_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    domain = 'https://www.transitionpathwayinitiative.org/publications'\n",
    "    start_url = domain\n",
    "    keywords = ['sovereign']\n",
    "    summary = crawl_website(start_url, domain, keywords)\n",
    "\n",
    "    print(\"\\nSummary of keyword findings:\")\n",
    "    for location, found_keywords in summary.items():\n",
    "        print(f\"Keywords found at {location}: {found_keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(start_url, domain, keywords, max_depth=0):\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = {start_url}\n",
    "    found_keywords_summary = {}  # Dictionary to store summary of findings\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop()\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        if 'mailto:' in current_url:\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Crawling: {current_url}\")\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code == 200:\n",
    "                if search_keywords_in_webpage(response.text, keywords):\n",
    "                    found_keywords_summary[current_url] = keywords\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    absolute_link = urljoin(domain, link['href'])\n",
    "                    if absolute_link not in visited_urls:\n",
    "                        urls_to_visit.add(absolute_link)\n",
    "\n",
    "                pdf_links = find_pdf_links(current_url, domain)\n",
    "                for pdf_link in pdf_links:\n",
    "                    pdf_io = download_pdf(pdf_link)\n",
    "                    if pdf_io and search_keywords_in_webpage(pdf_io, keywords):\n",
    "                        logging.info(f\"Found keywords in PDF: {pdf_link}\")\n",
    "                        found_keywords_summary[pdf_link] = keywords\n",
    "\n",
    "            visited_urls.add(current_url)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Error crawling {current_url}: {e}\")\n",
    "            visited_urls.add(current_url)\n",
    "\n",
    "    return found_keywords_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ba80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007041d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIN##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f46de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your parameters\n",
    "domain = 'https://joliennoels.com'\n",
    "start_url = domain\n",
    "keywords = ['greenhouse gas', 'net zero']\n",
    "\n",
    "# Start crawling\n",
    "crawl_website(start_url, domain, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5898098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da10798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915ba48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2cbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(start_url, domain, keywords):\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = {start_url}\n",
    "    domain_name = urlparse(domain).netloc\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop()\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        # Check if the URL belongs to the same domain\n",
    "        if urlparse(current_url).netloc != domain_name:\n",
    "            continue\n",
    "\n",
    "        print(f\"Crawling: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    # Complete URL\n",
    "                    absolute_link = urljoin(current_url, link['href'])\n",
    "\n",
    "                    # Skip if it's an email link or outside of domain\n",
    "                    if 'mailto:' in absolute_link or urlparse(absolute_link).netloc != domain_name:\n",
    "                        continue\n",
    "\n",
    "                    if absolute_link not in visited_urls:\n",
    "                        urls_to_visit.add(absolute_link)\n",
    "\n",
    "                # Here, you would continue with processing PDF links as before\n",
    "                # ...\n",
    "\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "\n",
    "        visited_urls.add(current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(start_url, domain, keywords):\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = {start_url}\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop()\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        if 'mailto:' in current_url:\n",
    "            continue\n",
    "\n",
    "        print(f\"Crawling: {current_url}\")\n",
    "\n",
    "        pdf_links = find_pdf_links(current_url, domain)\n",
    "        for pdf_link in pdf_links:\n",
    "            pdf_io = download_pdf(pdf_link)\n",
    "            if pdf_io and search_keywords_in_pdf(pdf_io, keywords):\n",
    "                print(f\"Found keywords in PDF: {pdf_link}\")\n",
    "\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        # Add new URLs to the queue (basic version, no check for same domain)\n",
    "        soup = BeautifulSoup(requests.get(current_url).text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            absolute_link = urljoin(domain, link['href'])\n",
    "            if absolute_link not in visited_urls:\n",
    "                urls_to_visit.add(absolute_link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
